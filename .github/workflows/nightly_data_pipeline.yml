name: Nightly Data Pipeline

on:
  schedule:
    # Runs every day at midnight UTC
    - cron: "0 0 * * *"
  workflow_dispatch:
    # Allows manual triggering of the workflow

env:
  AWS_REGION: us-east-1
  S3_BUCKET_NAME: baler-music-chatbot
  S3_DAILY_PREFIX: daily_scrapes
  LOCAL_OUTPUT_FILENAME: reviews_today.jsonl
  PREVIOUS_DAY_FILENAME: reviews_previous.jsonl
  POETRY_VERSION: "1.8.2" # Pin poetry version for consistency

jobs:
  scrape-and-build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: ${{ env.POETRY_VERSION }}
          virtualenvs-create: true
          virtualenvs-in-project: true
      
      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@v4
        with:
          path: .venv
          key: ${{ runner.os }}-poetry-${{ hashFiles('**/poetry.lock') }}
          restore-keys: |
            ${{ runner.os }}-poetry-

      - name: Install dependencies
        run: poetry install --no-interaction --no-root

      - name: Install Playwright browsers
        run: poetry run playwright install --with-deps chromium

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Find and download latest previous scrape
        id: download_previous
        run: |
          LATEST_FILE=$(aws s3api list-objects-v2 --bucket ${{ env.S3_BUCKET_NAME }} --prefix ${{ env.S3_DAILY_PREFIX }}/ --query 'sort_by(Contents, &LastModified)[-1].Key' --output text)
          if [ "$LATEST_FILE" = "null" ] || [ -z "$LATEST_FILE" ]; then
            echo "No previous scrape file found in S3. Performing full scrape."
            echo "previous_file_path=" >> $GITHUB_OUTPUT
          else
            echo "Latest file found: $LATEST_FILE"
            aws s3 cp s3://${{ env.S3_BUCKET_NAME }}/${LATEST_FILE} ${{ env.PREVIOUS_DAY_FILENAME }}
            echo "previous_file_path=${{ env.PREVIOUS_DAY_FILENAME }}" >> $GITHUB_OUTPUT
            echo "Downloaded previous day's data."
          fi

      - name: Run Scrapy Spider
        id: scrape
        run: |
          cd src
          PREVIOUS_ARG=""
          if [[ ! -z "${{ steps.download_previous.outputs.previous_file_path }}" ]]; then
            PREVIOUS_ARG="-a previous_file=../${{ steps.download_previous.outputs.previous_file_path }}"
          fi
          poetry run scrapy crawl pitchfork_reviews ${PREVIOUS_ARG} -o ../${{ env.LOCAL_OUTPUT_FILENAME }}

      - name: Upload today's reviews to S3
        if: steps.scrape.outcome == 'success'
        run: |
          if [ -s "${{ env.LOCAL_OUTPUT_FILENAME }}" ]; then
            TODAY=$(date +'%Y-%m-%d')
            S3_TARGET_PATH="s3://${{ env.S3_BUCKET_NAME }}/${{ env.S3_DAILY_PREFIX }}/reviews_${TODAY}.jsonl"
            echo "Uploading to ${S3_TARGET_PATH}"
            aws s3 cp ${{ env.LOCAL_OUTPUT_FILENAME }} ${S3_TARGET_PATH}
          else
            echo "Scraper output file is empty or does not exist. Skipping upload."
          fi
      
      - name: Configure GCP Credentials
        if: steps.scrape.outcome == 'success'
        run: |
          echo "${{ secrets.GCP_CREDENTIALS }}" | base64 --decode > ${{ github.workspace }}/gcloud-credentials.json
          echo "GOOGLE_APPLICATION_CREDENTIALS=${{ github.workspace }}/gcloud-credentials.json" >> $GITHUB_ENV

      - name: Build Knowledge Base from Today's Scrape
        if: steps.scrape.outcome == 'success'
        env:
          LLM_PROVIDER: GEMINI
          DB_PROVIDER: CLOUD
          CHROMA_CLOUD_API_KEY: ${{ secrets.CHROMA_CLOUD_API_KEY }}
          CHROMA_CLOUD_TENANT: ${{ secrets.CHROMA_CLOUD_TENANT }}
          CHROMA_CLOUD_DATABASE: ${{ secrets.CHROMA_CLOUD_DATABASE }}
        run: |
          if [ ! -s "${{ env.LOCAL_OUTPUT_FILENAME }}" ]; then
            echo "No new reviews were scraped. Skipping knowledge base build."
            exit 0
          fi
          poetry run python -m src.baler.create_knowledge_base --input-file ${{ env.LOCAL_OUTPUT_FILENAME }}
